= Implementierung
:toc:
:toc-title: Inhaltsverzeichnis
ifndef::main-file[]
:imagesdir: bilder
endif::main-file[]
ifdef::main-file[]
:imagesdir: prop-db/bilder
endif::main-file[]

== Systemvoraussetzungen

Alle Implementierungen wurden unter den folgenden Voraussetzungen vorgenommen:

* Windows 10 Home 64-bit
* Intel Core i7-1065G7
* 16 GB RAM

== Datenbank
=== Allgemein

Über ein relationales Datenbankmanagementsystem (RDBMS) kann die proprietäre Datenstruktur realisiert werden. Es wurde über XAMPP, einem kostenlosen Programmpaket von freier Software, welche meiste für ein Apache Webserver verwendet wird, ein MySQL Server 8.0 installiert und konfiguriert. +

.XAMPP
image::xampp.png[width=50%]

Um das relationale Datenmodell einfach zu realisieren, wurde zusätzlich HeidiSQL installiert, ein freier Client für das Datenbanksystem MySQL. Es wird hierbei die Structured Query Language (SQL) genutzt. +
 
.HeidiSQL
image::heidisql.png[width=80%]

Um dies zu verdeutlichen folgen Quelltextbeispiele. Diese zeigen wie eine Tabelle angelegt, Daten eingetragen, sowie Fremdschlüsselbeziehung erstellt werden.  

.Code 3 - Anlegen der Tabelle Database
[source, sql]
----
CREATE TABLE `Database` 
(
  baseID    int NOT NULL AUTO_INCREMENT,
  SNR 	    varchar(18) NOT NULL,
  FA	    varchar(12),
  TEIL	    varchar(12),
  LINIE     varchar(12),
  Fehler    varchar(12),
  Begintime datetime,
  Endtime   datetime,
  PRIMARY KEY (baseID)
);
----

.Code 4 - Daten in Database eintragen
[source, sql]
----
INSERT INTO `Database` (`SNR`,`FA`,`TEIL`,`LINIE`,`Fehler`,`Begintime`,`Endtime`)
    VALUES ('1923219423129', '009606', 'A', '1', '0', '2018-01-02T05:47:45', '2018-01-02T05:48:45');
----

.Code 5 - Fremdschlüsselbeziehung anlegen
[source, sql]
----
ALTER TABLE `Database` ADD CONSTRAINT FKDatabase FOREIGN KEY (`baseID`) REFERENCES `Input` (`baseID`);
----

Um gezielte Abfragen zu tätigen, die später für die Analyse benötigt werden, bedarf es ein SELECT-STATEMENT mit einer WHERE-CLAUSEL. Dadurch wird eine gute Performance gesichert.

.Code 6 - SELECT-Statement inkl. WHERE-Clausel
[source, sql]
----
SELECT * FROM `Database` WHERE `baseID` = `2`
----

=== Messung der Ausführungszeit

Um einen Vergleich der Abfragezeit zu gewährleisten, ist es nötig die Zeit zu messen welche die Datenbank braucht, um eine Abfrage durchzuführen. Dies ist über den folgenden Befehl zu realisieren.

.Code 7 - Abfragezeit messen
[source, sql]
----
SET STATISTICS TIME ON
----

== Anwendungen
=== Programmiersprache

Die Implementierung der Anwendungen wurde mit der Programmiersprache Python vorgenommen. Als Entwicklungsumgebung, innerhalb des Projektes, wurde Notepad ++ genutzt.  Für die Implementierung wurde die Python-Version 3.7.3 genutzt. +
Die Ausführung des Python-Programmes wurde über die Kommandozentrale (CMD), von Windows 10, vorgenommen. +
Dies wird in im Codebeispiel 8 verdeutlicht. 

.Code 8 - Ausführung des Python-Programmes über CMD
[source, sh]
----
>python analyse.py 
----

=== Kommunikation zwischen Anwendung und Datenbank

Durch die Python-Bibliothek mysql-connector-python wurde eine Verbindung zwischen Anwendung und MySQL-Datenbankserver hergestellt. +
Dies kann man über die Kommandozentrale installieren, wie im Codebeispiel 9 beschrieben.

.Code 9 - Installation Python-Bibliothek über CMD
[source, sh]
----
>pip install mysql-connector-python
----

Die installierte Bibliothek wird nun in die Anwendung eingebunden, um eine Verbindung zur Datenbank herstellen zu können. Dazu müssen folgende Parameter gesetzt werden um einen Zugriff zu erhalten: host, database, user und password.

.Code 10 - Herstellung der Verbindung zwischen Anwendung und Datenbank
[source, python]
----
import mysql.connector

connection = mysql.connector.connect(host = "127.0.0.1", user = "root", password = "1234", database = "test")
----

Nach dem Aufbau der Verbindung zur Datenbank, wird ein Cursor gesetzt. Diesem wird per Cursor-Funktion execute eine SELECT-Abfrage übergeben. Zusätzlich dazu bietet der Cursor Varianten an, um die Ergebnismenge bereitzustellen. Zum Beispiel:

* _fetchall()_, welcher die gesamte Ergebnismenge wiedergibt
* _fetchone()_, welcher nur die erste Zeile der Ergebnismenge wiedergibt

.Code 11 - Erstellung eines Cursor's sowie einer SELECT-Abfrage
[source, python]
----
cursor = connection.cursor()
cursor.execute("SELECT * FROM `Database`")
result = cursor.fetchall()
----

Um die execute auszuführen, bedarf es ein Commit, welches die Anweisung ausführt.  Nach dem Ausführen, wird der Cursor, sowie die Connection per close() Funktion geschlossen.

.Code 12 - Commit und Close der Connection
[source, python]
----
connection.commit()

connection.close()
cursor.close()
----

== Datenloader

Die Voraussetzung für den Datenloader ist eine Textdatei, welche alle Informationen für den Input bzw. Output enthält. Das Vorgehen für die Input- und Output-Sätze ist im Allgemeinen gleich. Der Unterschied liegt in der Überprüfung, des zeitigen Starttermins (Begintime) und des spätesten Endtermins (Endtime). Dies wird in Abbildung 18 virtualisiert. +

Die Textdateien werden über den Watchdog, welcher bereits erläutert wurde, zur Verfügung gestellt. Das Einlesen der Dateien wird über den folgenden Pythoncode dargestellt. In diesem wird das Dokument geöffnet und in einen String verpackt. Dies ermöglicht es die erhalten Informationen über ein Insert-Statement in die Datenbank zu speichern.

.Code 13 - Daten auslesen
[source, python]
----
datei   = open(filename, 'r')
val     = dati.read() 
dat     = val.split(';')
----

.Einlese-Algorithmus
image::einlesen.png[]

Zunächst wird eine Textdatei eingelesen, in einen String verpackt und auf Vollständigkeit geprüft. Wenn die Vollständigkeit der Daten nicht gewährt ist, wird der Datensatz mit einem Fehlercode vermerkt und in der Input- bzw. Output- Tabelle gespeichert. Nachdem die Überprüfung positiv abgelaufen ist, vergleicht der Algorithmus nun ob die Seriennummer (SNR) in der Database-Tabelle vorhanden ist. +

Nachdem keine SNR gefunden wurde, wird der Datensatz in der Database- sowie Input- oder Output- Tabelle gespeichert, die baseID stellt dabei die Verknüpfung der Tabellen her. Der andere Fall wäre, dass eine SNR in der Database-Tabelle schon vorhanden ist. In dem Zusammenhang wird der Zeitstempel überprüft, im Fall des Inputdatensatzes der Startzeitpunkt. Wenn dieser kleiner ist als das bisherige Datum, wird er ersetzt. Die restlichen Datensätze werden ebenso in der Input- bzw. Output- Tabelle gespeichert und miteinander verknüpft.


== Analysen

=== Allgemein

Zunächst wurde über die Möglichkeiten der Implementierung der Analysen diskutiert. Dafür gab es im Allgemeinen drei Möglichkeiten. +

Die erste Möglichkeit sah vor, alle Analysen in SQL zu schreiben und über HeidiSQL auszuführen. Die gewonnen Daten hätten man in MS Excel importieren können, um diese darin auszuwerten und grafisch darzustellen. Diese Möglichkeit wurde zunächst in Betracht gezogen, eine zuverlässige Umsetzung wurde mit dieser Methode generiert. Allerdings war diese auch durch den Wechsel zwischen HeidiSQL und MS Excel zeitaufwendig. +
Um dieses Problem zu lösen, wurde Möglichkeit zwei eingeführt. Mit Devart, einem freiverfügbaren Data Integration Tool, konnte eine Abfrage direkt in Excel ausgeführt werden. Mit der Methode konnte eine Zeitersparnis festgestellt werden, die durch das Wegfallen von HeidiSQL zustande gekommen ist. Das Problem hierbei war die Datenintegration mit den anderen Datenstrukturen. +
Möglichkeit drei ist die Abfrage der Daten mit Python, wie es in der Anwendung beschrieben ist. Die erfolgreiche Analyse wurde in einer CSV-Datei gespeichert. Mit dieser konnte die Analyse erfolgreich durchgeführt und mit den anderen Datenstrukturen verglichen werden. +

Im Allgemeinen hat die Vorarbeit mit Möglichkeit eins und zwei die Entwicklung der finalen Lösung beschleunigt. Um dies zu verdeutlichen befinden sich im Nachfolgenden die Analyseabfragen mit Pseudocode.  


=== Analyse 1 

.Code 14 - Taktung pro Artikel
[source, sql]
----
SELECT teil, fa, COUNT(baseid) AS COUNT, 
  MIN(UNIX_TIMESTAMP(endtime)-UNIX_TIMESTAMP(begintime)) AS MIN, 
  MAX(UNIX_TIMESTAMP(endtime)-UNIX_TIMESTAMP(begintime)) AS MAX,  
  AVG(UNIX_TIMESTAMP(endtime)-UNIX_TIMESTAMP(begintime))AS avg 
  FROM `database` 
  WHERE endtime is not null  
  GROUP BY FA 
  ORDER BY teil, fa
----

=== Analyse 2 

.Code 15 - Auftrennung
[source, sql]
----
SELECT `TEIL`, 
  COUNT(snrid) AS Anzahl, 
  min(unix_timestamp(endtime) - unix_timestamp(Begintime)) AS MinFertigungszeit, 
  max(unix_timestamp(endtime) - unix_timestamp(Begintime)) AS MaxFertigungszeit, 
  avg(unix_timestamp(endtime) - unix_timestamp(Begintime)) AS AVGFertigungszeit 
  FROM `database` 
  WHERE endtime IS NOT NULL  
  GROUP BY `TEIL`
----

=== Analyse 4 

.Code 16 - Analyse 4
[source, sql]
----
SELECT a.Lagerin, 
  max(unix_timestamp(b.endtime))-MIN(unix_timestamp(a.Begintime)) AS Dauer, 
  min(a.Begintime) AS Start, 
  max(b.endtime) AS Ende, 
  COUNT(b.baseID) 
  FROM basedatain2 AS a JOIN basedata AS b ON a.baseID = b.baseID 
  where a.lagerout > 0 
  GROUP BY a.Lagerin
----

=== Analyse 5

.Code 17 - Analyse 5
[source, sql]
----
SELECT a.TEIL, a.Lagerin, 
  count(b.baseid), 
  (min(unix_timestamp(b.endtime)-unix_timestamp(b.begintime))) AS MinFertigungszeit, 
  (max(unix_timestamp(b.endtime)-unix_timestamp(b.begintime))) AS MaxFertigungszeit, 
  (avg(unix_timestamp(b.endtime)-unix_timestamp(b.begintime))) AS AVGFertigungszeit 
  FROM `basedatain2` as a Join basedata as b on a.baseid=b.baseid 
  WHERE b.endtime IS NOT NULL  AND a.Lagerout > 0  
  GROUP BY a.lagerin 
  ORDER BY a.TEIL
----

=== Analyse 6

.Code 18 - Analyse 6 - Pseudocode
[source, python]
----
Select `LINIE` from basedatain2 group by LINIE ORDER BY linie asc

Linies in row umwandeln

SELECT fa, teil, min(unix_timestamp(begintime)), 
Max(unix_timestamp(begintime)) 
from basedatain2 WHERE LINIE = '"+row[0]+"' group by FA order by begintime asc

diff = dauer
if str(diff) >= str(0):           
    if index == -1:
          * aktuelle Länge der Ergebnisliste berechnen
          * neues Element am Ende der Ergebnisliste einfügen 
    else:
          * vorhandene Werte aus Liste auslesen
          * Prüfen, ob min/max verändert werden müssen
          * Dauer zur AVG Berechnung hinzufügen
          * Dauer zurück in Liste schreiben
else:
    break
----

=== Messung der Ausführungszeit

Um die Messung der Ausführungszeit der Analysen in Python zu realisieren, wurde die Zeit vor und nach der Ausführung gemessen. Die Differenz zwischen Start und Ende ergibt die Ausführungszeit und wird in Sekunden zurückgegeben. 

.Code 19 - Ausführungszeit messen
[source, sql]
----
import time

start = time.time()
#Ausführen der Analyse
ende = time.time()

print('{:5.3f}s'.format(ende-start))
----

== Auswertung
=== Allgemein


Nach der Auswertung der Analysezeiten (Prozesszeiten) bzw. der Antwortzeiten von MySQL, wurde in diesem Zusammenhang eine Grafik in MS Excel erstellt. Diese zeigt deutlich, dass die Antwortzeiten in MySQL kürzer sind als die Prozesszeiten in Python. Grund dafür ist Datenverarbeitung nach der Abfrage in Python, dies kostet zusätzlich mehr Zeit.

.Prozesszeiten
image::prozesszeiten.png[]

=== Lessons Learned

Schlussendlich erhält man mit der proprietären Datenstruktur eine schnelle und zuverlässige Auswertung der eingelesenen Daten. Jedoch erkennt man bei dieser Struktur, dass eine Weiterentwicklung nur mit mehr Zeitaufwand möglich ist. +

Für eine individuelle und schnelle Lösung ist dieses Modell gut geeignet.
